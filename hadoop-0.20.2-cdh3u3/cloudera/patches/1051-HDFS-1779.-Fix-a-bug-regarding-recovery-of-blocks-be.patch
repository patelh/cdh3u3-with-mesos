From 81caf3bf3a5099a7b15a6f55aae930d7beb96a0c Mon Sep 17 00:00:00 2001
From: Todd Lipcon <todd@cloudera.com>
Date: Sun, 18 Sep 2011 10:14:23 -0700
Subject: [PATCH 1051/1117] HDFS-1779. Fix a bug regarding recovery of blocks being written while NN restarts

This patch adds a new RPC 'blocksBeingWrittenReport()' which the DN calls
on startup and when it re-connects to a restarted NameNode. This reports
all blocks currently under construction, so the NN can re-add them to
the targets list for a block if necessary.

Reason: avoid HBase data-loss scenario when NN crashes
Author: Hairong Kuang
Ref: CDH-3507
---
 .../hadoop/hdfs/server/datanode/DataNode.java      |    9 ++-
 .../hadoop/hdfs/server/datanode/FSDataset.java     |  121 ++++++++++++++------
 .../hdfs/server/datanode/FSDatasetInterface.java   |    6 +
 .../hadoop/hdfs/server/namenode/FSNamesystem.java  |   70 +++++++++++-
 .../hadoop/hdfs/server/namenode/NameNode.java      |   15 +++
 .../hdfs/server/protocol/DatanodeProtocol.java     |   11 ++
 .../org/apache/hadoop/hdfs/MiniDFSCluster.java     |   15 ++-
 .../hdfs/server/datanode/SimulatedFSDataset.java   |    6 +-
 .../hdfs/server/namenode/TestBBWBlockReport.java   |  118 +++++++++++++++++++
 9 files changed, 332 insertions(+), 39 deletions(-)
 create mode 100644 src/test/org/apache/hadoop/hdfs/server/namenode/TestBBWBlockReport.java

diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java
index f210d38..fe2730a 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java
@@ -216,6 +216,7 @@ public class DataNode extends Configured
   int socketWriteTimeout = 0;  
   boolean transferToAllowed = true;
   int writePacketSize = 0;
+  private boolean supportAppends;
 
   /**
    * Testing hook that allows tests to delay the sending of blockReceived
@@ -275,7 +276,7 @@ public class DataNode extends Configured
         DFSConfigKeys.DFS_DATANODE_USER_NAME_KEY);
 
     datanodeObject = this;
-
+    supportAppends = conf.getBoolean("dfs.support.append", true);
     try {
       startDataNode(conf, dataDirs, resources);
     } catch (IOException ie) {
@@ -702,6 +703,12 @@ public class DataNode extends Configured
       dnRegistration.exportedKeys = ExportedBlockKeys.DUMMY_KEYS;
     }
 
+    if (supportAppends) {
+      Block[] bbwReport = data.getBlocksBeingWrittenReport();
+      long[] blocksBeingWritten = BlockListAsLongs
+          .convertToArrayLongs(bbwReport);
+      namenode.blocksBeingWrittenReport(dnRegistration, blocksBeingWritten);
+    }
     // random short delay - helps scatter the BR from all DNs
     scheduleBlockReport(initialBlockReportDelay);
   }
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java
index 62ac610..5703ba6 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java
@@ -60,6 +60,33 @@ import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;
  *
  ***************************************************/
 public class FSDataset implements FSConstants, FSDatasetInterface {
+  
+
+  /** Find the metadata file for the specified block file.
+   * Return the generation stamp from the name of the metafile.
+   */
+  private static long getGenerationStampFromFile(File[] listdir, File blockFile) {
+    String blockName = blockFile.getName();
+    for (int j = 0; j < listdir.length; j++) {
+      String path = listdir[j].getName();
+      if (!path.startsWith(blockName)) {
+        continue;
+      }
+      String[] vals = path.split("_");
+      if (vals.length != 3) {     // blk, blkid, genstamp.meta
+        continue;
+      }
+      String[] str = vals[2].split("\\.");
+      if (str.length != 2) {
+        continue;
+      }
+      return Long.parseLong(str[0]);
+    }
+    DataNode.LOG.warn("Block " + blockFile + 
+                      " does not have a metafile!");
+    return Block.GRANDFATHER_GENERATION_STAMP;
+  }
+
 
   /**
    * A data structure than encapsulates a Block along with the full pathname
@@ -184,31 +211,6 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
       return children[ lastChildIdx ].addBlock(b, src, true, false); 
     }
 
-    /** Find the metadata file for the specified block file.
-     * Return the generation stamp from the name of the metafile.
-     */
-    long getGenerationStampFromFile(File[] listdir, File blockFile) {
-      String blockName = blockFile.getName();
-      for (int j = 0; j < listdir.length; j++) {
-        String path = listdir[j].getName();
-        if (!path.startsWith(blockName)) {
-          continue;
-        }
-        String[] vals = path.split("_");
-        if (vals.length != 3) {     // blk, blkid, genstamp.meta
-          continue;
-        }
-        String[] str = vals[2].split("\\.");
-        if (str.length != 2) {
-          continue;
-        }
-        return Long.parseLong(str[0]);
-      }
-      DataNode.LOG.warn("Block " + blockFile + 
-                        " does not have a metafile!");
-      return Block.GRANDFATHER_GENERATION_STAMP;
-    }
-
     /**
      * Populate the given blockSet with any child blocks
      * found at this node.
@@ -224,7 +226,7 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
       if (blockFiles != null) {
         for (int i = 0; i < blockFiles.length; i++) {
           if (Block.isBlockFilename(blockFiles[i])) {
-            long genStamp = getGenerationStampFromFile(blockFiles,
+            long genStamp = FSDataset.getGenerationStampFromFile(blockFiles,
                 blockFiles[i]);
             blockSet.add(new Block(blockFiles[i], blockFiles[i].length(),
                 genStamp));
@@ -248,7 +250,7 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
       File blockFiles[] = dir.listFiles();
       for (int i = 0; i < blockFiles.length; i++) {
         if (Block.isBlockFilename(blockFiles[i])) {
-          long genStamp = getGenerationStampFromFile(blockFiles, blockFiles[i]);
+          long genStamp = FSDataset.getGenerationStampFromFile(blockFiles, blockFiles[i]);
           Block block = new Block(blockFiles[i], blockFiles[i].length(), genStamp);
           blockSet.add(new BlockAndFile(blockFiles[i].getAbsoluteFile(), block));
         }
@@ -266,7 +268,7 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
       if (blockFiles != null) {
         for (int i = 0; i < blockFiles.length; i++) {
           if (Block.isBlockFilename(blockFiles[i])) {
-            long genStamp = getGenerationStampFromFile(blockFiles,
+            long genStamp = FSDataset.getGenerationStampFromFile(blockFiles,
                 blockFiles[i]);
             volumeMap.put(new Block(blockFiles[i], blockFiles[i].length(),
                 genStamp), new DatanodeBlockInfo(volume, blockFiles[i]));
@@ -386,7 +388,7 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
       // should not be deleted.
       blocksBeingWritten = new File(parent, "blocksBeingWritten");
       if (blocksBeingWritten.exists()) {
-        if (supportAppends) {
+        if (supportAppends) {  
           recoverBlocksBeingWritten(blocksBeingWritten);
         } else {
           FileUtil.fullyDelete(blocksBeingWritten);
@@ -526,6 +528,35 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
     void getBlockInfo(TreeSet<Block> blockSet) {
       dataDir.getBlockInfo(blockSet);
     }
+    
+    void getBlocksBeingWrittenInfo(TreeSet<Block> blockSet) {
+      if (blocksBeingWritten == null) {
+        return;
+      }
+      
+      File[] blockFiles = blocksBeingWritten.listFiles();
+      if (blockFiles == null) {
+        return;
+      }
+      
+      for (int i = 0; i < blockFiles.length; i++) {
+        if (!blockFiles[i].isDirectory()) {
+          // get each block in the blocksBeingWritten direcotry
+          if (Block.isBlockFilename(blockFiles[i])) {
+            long genStamp = 
+              FSDataset.getGenerationStampFromFile(blockFiles, blockFiles[i]);
+            Block block = 
+              new Block(blockFiles[i], blockFiles[i].length(), genStamp);
+            
+            // add this block to block set
+            blockSet.add(block);
+            if (DataNode.LOG.isDebugEnabled()) {
+              DataNode.LOG.debug("recoverBlocksBeingWritten for block " + block);
+            }
+          }
+        }
+      }
+    }
 
     void getVolumeMap(HashMap<Block, DatanodeBlockInfo> volumeMap) {
       dataDir.getVolumeMap(volumeMap, this);
@@ -542,9 +573,7 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
     /**
      * Recover blocks that were being written when the datanode
      * was earlier shut down. These blocks get re-inserted into
-     * ongoingCreates. Also, send a blockreceived message to the NN
-     * for each of these blocks because these are not part of a 
-     * block report.
+     * ongoingCreates.
      */
     private void recoverBlocksBeingWritten(File bbw) throws IOException {
       FSDir fsd = new FSDir(bbw);
@@ -557,8 +586,6 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
         if (DataNode.LOG.isDebugEnabled()) {
           DataNode.LOG.debug("recoverBlocksBeingWritten for block " + b.block);
         }
-        DataNode.getDataNode().notifyNamenodeReceivedBlock(b.block, 
-                               DataNode.EMPTY_DEL_HINT);
       }
     } 
     
@@ -673,6 +700,18 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
         volumes[idx].getVolumeMap(volumeMap);
       }
     }
+    
+    synchronized void getBlocksBeingWrittenInfo(TreeSet<Block> blockSet) {
+      long startTime = System.currentTimeMillis();
+
+      for (int idx = 0; idx < volumes.length; idx++) {
+        volumes[idx].getBlocksBeingWrittenInfo(blockSet);
+      }
+      
+      long scanTime = (System.currentTimeMillis() - startTime)/1000;
+      DataNode.LOG.info("Finished generating blocks being written report for " +
+          volumes.length + " volumes in " + scanTime + " seconds");
+    }
       
     /**
      * goes over all the volumes and checkDir eachone of them
@@ -1618,6 +1657,20 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
   }
   
   /**
+   * Return a table of blocks being written data
+   */
+  public Block[] getBlocksBeingWrittenReport() {
+    TreeSet<Block> blockSet = new TreeSet<Block>();
+    volumes.getBlocksBeingWrittenInfo(blockSet);
+    Block blockTable[] = new Block[blockSet.size()];
+    int i = 0;
+    for (Iterator<Block> it = blockSet.iterator(); it.hasNext(); i++) {
+      blockTable[i] = it.next();
+    }
+    return blockTable;
+  }
+  
+  /**
    * Return a table of block data
    */
   public Block[] getBlockReport() {
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java
index 4497a46..0e17105 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java
@@ -234,6 +234,12 @@ public interface FSDatasetInterface extends FSDatasetMBean {
    * @return - the block report - the full list of blocks stored
    */
   public Block[] getBlockReport();
+  
+  /**
+   * Returns the blocks being written report 
+   * @return - the blocks being written report
+   */
+  public Block[] getBlocksBeingWrittenReport();
 
   /**
    * Is the block valid?
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java b/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
index 5d79d6e..facfa23 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
@@ -3278,7 +3278,75 @@ public class FSNamesystem implements FSConstants, FSNamesystemMBean,
       allAlive = !foundDead;
     }
   }
-    
+  
+  /**
+   * It will update the targets for INodeFileUnderConstruction
+   * 
+   * @param nodeID
+   *          - DataNode ID
+   * @param blocksBeingWritten
+   *          - list of blocks which are still inprogress.
+   * @throws IOException
+   */
+  public synchronized void processBlocksBeingWrittenReport(DatanodeID nodeID,
+      BlockListAsLongs blocksBeingWritten) throws IOException {
+    DatanodeDescriptor dataNode = getDatanode(nodeID);
+    if (dataNode == null) {
+      throw new IOException("ProcessReport from unregistered node: "
+          + nodeID.getName());
+    }
+
+    // Check if this datanode should actually be shutdown instead.
+    if (shouldNodeShutdown(dataNode)) {
+      setDatanodeDead(dataNode);
+      throw new DisallowedDatanodeException(dataNode);
+    }
+
+    Block block = new Block();
+
+    for (int i = 0; i < blocksBeingWritten.getNumberOfBlocks(); i++) {
+      block.set(blocksBeingWritten.getBlockId(i), blocksBeingWritten
+          .getBlockLen(i), blocksBeingWritten.getBlockGenStamp(i));
+
+      BlockInfo storedBlock = blocksMap.getStoredBlockWithoutMatchingGS(block);
+
+      if (storedBlock == null) {
+        rejectAddStoredBlock(new Block(block), dataNode,
+            "Block not in blockMap with any generation stamp");
+        continue;
+      }
+
+      INodeFile inode = storedBlock.getINode();
+      if (inode == null) {
+        rejectAddStoredBlock(new Block(block), dataNode,
+            "Block does not correspond to any file");
+        continue;
+      }
+
+      boolean underConstruction = inode.isUnderConstruction();
+      boolean isLastBlock = inode.getLastBlock() != null
+          && inode.getLastBlock().getBlockId() == block.getBlockId();
+
+      // Must be the last block of a file under construction,
+      if (!underConstruction) {
+        rejectAddStoredBlock(new Block(block), dataNode,
+            "Reported as block being written but is a block of closed file.");
+        continue;
+      }
+
+      if (!isLastBlock) {
+        rejectAddStoredBlock(new Block(block), dataNode,
+            "Reported as block being written but not the last block of "
+                + "an under-construction file.");
+        continue;
+      }
+
+      INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction) inode;
+      pendingFile.addTarget(dataNode);
+      incrementSafeBlockCount(pendingFile.getTargets().length);
+    }
+  }
+  
   /**
    * The given node is reporting all its blocks.  Use this info to 
    * update the (machine-->blocklist) and (block-->machinelist) tables.
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java b/src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java
index 873b927..272d2d1 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java
@@ -951,6 +951,21 @@ public class NameNode implements ClientProtocol, DatanodeProtocol,
       return DatanodeCommand.FINALIZE;
     return null;
   }
+  
+  /**
+   * add new replica blocks to the Inode to target mapping
+   * also add the Inode file to DataNodeDesc
+   */
+  public void blocksBeingWrittenReport(DatanodeRegistration nodeReg,
+      long[] blocks) throws IOException {
+    verifyRequest(nodeReg);
+    BlockListAsLongs blist = new BlockListAsLongs(blocks);
+    namesystem.processBlocksBeingWrittenReport(nodeReg, blist);
+    
+    stateChangeLog.info("*BLOCK* NameNode.blocksBeingWrittenReport: "
+           +"from "+nodeReg.getName()+" "+blocks.length +" blocks");
+    
+  }
 
   public void blockReceived(DatanodeRegistration nodeReg, 
                             Block blocks[],
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java b/src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java
index d11a897..3b75803 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java
@@ -108,6 +108,17 @@ public interface DatanodeProtocol extends VersionedProtocol {
    */
   public DatanodeCommand blockReport(DatanodeRegistration registration,
                                      long[] blocks) throws IOException;
+  
+  /**
+   * blocksBeingWrittenReport() tells the NameNode about the blocks-being-
+   * written information
+   * 
+   * @param registration
+   * @param blocks
+   * @throws IOException
+   */
+  public void blocksBeingWrittenReport(DatanodeRegistration registration,
+      long[] blocks) throws IOException;
     
   /**
    * blockReceived() allows the DataNode to tell the NameNode about
diff --git a/src/test/org/apache/hadoop/hdfs/MiniDFSCluster.java b/src/test/org/apache/hadoop/hdfs/MiniDFSCluster.java
index d28f2a4..7547b35 100644
--- a/src/test/org/apache/hadoop/hdfs/MiniDFSCluster.java
+++ b/src/test/org/apache/hadoop/hdfs/MiniDFSCluster.java
@@ -583,12 +583,23 @@ public class MiniDFSCluster {
   }
 
   /**
+   * Restart namenode. Waits for exit from safemode.
+   */
+  public synchronized void restartNameNode()
+      throws IOException {
+    restartNameNode(true);
+  }
+  
+  /**
    * Restart namenode.
    */
-  public synchronized void restartNameNode() throws IOException {
+  public synchronized void restartNameNode(boolean waitSafemodeExit)
+      throws IOException {
     shutdownNameNode();
     nameNode = NameNode.createNameNode(new String[] {}, conf);
-    waitClusterUp();
+    if (waitSafemodeExit) {
+      waitClusterUp();
+    }
     System.out.println("Restarted the namenode");
     int failedCount = 0;
     while (true) {
diff --git a/src/test/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java b/src/test/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
index 97d0546..5201fbd 100644
--- a/src/test/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
+++ b/src/test/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
@@ -17,7 +17,6 @@
  */
 package org.apache.hadoop.hdfs.server.datanode;
 
-import java.io.File;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
@@ -700,4 +699,9 @@ public class SimulatedFSDataset  implements FSConstants, FSDatasetInterface, Con
     Block stored = getStoredBlock(blockId);
     return new BlockRecoveryInfo(stored, false);
   }
+
+  @Override
+  public Block[] getBlocksBeingWrittenReport() {
+    return new Block[0];
+  }
 }
diff --git a/src/test/org/apache/hadoop/hdfs/server/namenode/TestBBWBlockReport.java b/src/test/org/apache/hadoop/hdfs/server/namenode/TestBBWBlockReport.java
new file mode 100644
index 0000000..b29ec70
--- /dev/null
+++ b/src/test/org/apache/hadoop/hdfs/server/namenode/TestBBWBlockReport.java
@@ -0,0 +1,118 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.namenode;
+
+import static org.junit.Assert.assertEquals;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.hdfs.protocol.FSConstants.SafeModeAction;
+import org.apache.hadoop.io.IOUtils;
+import org.junit.Before;
+import org.junit.Test;
+
+public class TestBBWBlockReport {
+
+  private final Path src = new Path(System.getProperty("test.build.data",
+      "/tmp"), "testfile");
+
+  private Configuration conf = null;
+
+  private final String fileContent = "PartialBlockReadTest";
+
+  @Before
+  public void setUp() {
+    conf = new Configuration();
+    conf.setInt("ipc.client.connection.maxidletime", 1000);
+  }
+
+  @Test(timeout = 60000)
+  // timeout is mainly for safe mode
+  public void testDNShouldSendBBWReportIfAppendOn() throws Exception {
+    FileSystem fileSystem = null;
+    FSDataOutputStream outStream = null;
+    conf.setBoolean("dfs.support.append", true);
+    MiniDFSCluster cluster = new MiniDFSCluster(conf, 1, true, null);
+    cluster.waitActive();
+    try {
+      fileSystem = cluster.getFileSystem();
+      // Keep open stream
+      outStream = writeFileAndSync(fileSystem, src, fileContent);
+      // Parameter true will ensure that NN came out of safemode
+      cluster.restartNameNode();
+      assertEquals(
+          "Not able to read the synced block content after NameNode restart (with append support)",
+          fileContent, getFileContentFromDFS(fileSystem));
+    } finally {
+      if (null != fileSystem)
+        fileSystem.close();
+      if (null != outStream)
+        outStream.close();
+      cluster.shutdown();
+    }
+  }
+
+  @Test
+  public void testDNShouldNotSendBBWReportIfAppendOff() throws Exception {
+    FileSystem fileSystem = null;
+    FSDataOutputStream outStream = null;
+    // disable the append support
+    conf.setBoolean("dfs.support.append", false);
+    MiniDFSCluster cluster = new MiniDFSCluster(conf, 1, true, null);
+    cluster.waitActive();
+    try {
+      fileSystem = cluster.getFileSystem();
+      // Keep open stream
+      outStream = writeFileAndSync(fileSystem, src, fileContent);
+      cluster.restartNameNode(false);
+      Thread.sleep(2000);
+      assertEquals(
+          "Able to read the synced block content after NameNode restart (without append support",
+          0, getFileContentFromDFS(fileSystem).length());
+    } finally {
+      // NN will not come out of safe mode. So exited the safemode forcibly to
+      // clean the resources.
+      cluster.getNameNode().setSafeMode(SafeModeAction.SAFEMODE_LEAVE);
+      if (null != fileSystem)
+        fileSystem.close();
+      if (null != outStream)
+        outStream.close();
+      cluster.shutdown();
+    }
+  }
+
+  private String getFileContentFromDFS(FileSystem fs) throws IOException {
+    ByteArrayOutputStream bio = new ByteArrayOutputStream();
+    IOUtils.copyBytes(fs.open(src), bio, conf, true);
+    return new String(bio.toByteArray());
+  }
+
+  private FSDataOutputStream writeFileAndSync(FileSystem fs, Path src,
+      String fileContent) throws IOException {
+    FSDataOutputStream fo = fs.create(src);
+    fo.writeBytes(fileContent);
+    fo.sync();
+    return fo;
+  }
+}
-- 
1.7.0.4

